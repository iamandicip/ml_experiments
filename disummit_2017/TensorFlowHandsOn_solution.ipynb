{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tensorflow 101\n",
    "\n",
    "\n",
    "## 0. Introduction to Jupyter Notebook\n",
    "\n",
    "Jupyter notebook is often used by data scientists who work in Python. It is loosely based on Mathematica and combines code, text and visual output in one page.\n",
    "\n",
    "## 0.1 Jupyter Notebook short cuts\n",
    "\n",
    "\n",
    "Some relevant short cuts:\n",
    "* ```SHIFT + ENTER``` executes 1 block of code called a cell\n",
    "* Tab completion is omnipresent after the import of a package has been executed\n",
    "* ```SHIFT + TAB``` gives you extra information on what parameters a function takes\n",
    "* Repeating ```SHIFT + TAB``` multiple times gives you even more information\n",
    "\n",
    "To get used to these short cuts try them out on the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print 'Hello world!'\n",
    "print range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "## 1. Importing The Data\n",
    "\n",
    "\n",
    "Let's start with importing and exploring the data we'll be using.\n",
    "\n",
    "We will work with the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset for immage classification. The data consists of images of handwritten digits ranging from 0 to 9. Here mnist is a lightweight class which stores the training, validation, and testing sets as NumPy arrays:\n",
    "\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are 55000 train images each consisting of 28x28 = 784 pixels\n",
    "and 10 classes (the numbers 0 to 9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.shape(mnist.train.images))\n",
    "print(np.shape(mnist.train.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we have 10000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(mnist.test.images))\n",
    "print(np.shape(mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "---\n",
    "## 1.1 Exploring The Data\n",
    "\n",
    "\n",
    "Let's take a peek at the data to make sure it looks sensible. Each exemplar is an image of a handwritten digit from 0 through 9. We'll display some samples of the images that we just downloaded using matplotlib.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "our labels are one hot encoded vectors of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADfCAYAAADmzyjKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c485b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "for i, img in enumerate(mnist.train.images[0:10]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    \n",
    "    # the one hot encoded vectors have a '1' at the position of the class\n",
    "    plt.title(np.nonzero(mnist.train.labels[i])[0])\n",
    "    plt.axis('off')\n",
    "    img = np.reshape(img,[28,28])\n",
    "\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (55000, 784), (55000, 10))\n",
      "('Validation set', (5000, 784), (5000, 10))\n",
      "('Test set', (10000, 784), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(mnist.train.labels[0])\n",
    "\n",
    "train_dataset, train_labels = mnist.train.images, mnist.train.labels\n",
    "valid_dataset, valid_labels = mnist.validation.images, mnist.validation.labels\n",
    "test_dataset, test_labels = mnist.test.images, mnist.test.labels\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L8oww1s4JMQx"
   },
   "source": [
    "## 1.2 Benchmark Classifier\n",
    "\n",
    "Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it.\n",
    "We'll use a RandomForest classifier from sklearn as benchmark classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=20,min_samples_split=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC.fit(train_dataset,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  88.84 %\n"
     ]
    }
   ],
   "source": [
    "test_prediction = RFC.predict(test_dataset)\n",
    "print 'test accuracy: ',accuracy(test_prediction,test_labels) , '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That's already not too bad! Let's see if we can improve this with deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "To do efficient numerical computing in Python, we typically use libraries like NumPy that do expensive operations such as matrix multiplication outside Python, using highly efficient code implemented in another language. Unfortunately, there can still be a lot of overhead from switching back to Python every operation. This overhead is especially bad if you want to run computations on GPUs or in a distributed manner, where there can be a high cost to transferring data.\n",
    "\n",
    "TensorFlow also does its heavy lifting outside Python, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from Python, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. The role of the Python code is therefore to build this external computation graph, and to dictate which parts of the computation graph should be run. \n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 Implementing Multinomial Logistic Regression in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1.1 Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll build a multinomial logistic regression model in tensorflow. Here for each training step all the training examples will be presented to the model and the loss will be calculated over all these training samples. The weights will then be updated according to this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "\n",
    "# square images of 28 pixels width and 28 pixels heigt --> 28*28 =784 pixels\n",
    "image_size = 28 \n",
    "\n",
    "# 10 different output classes\n",
    "num_labels = train_labels.shape[1] \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 14.785463\n",
      "Training accuracy: 7.1%\n",
      "Validation accuracy: 6.9%\n",
      "Test accuracy: 7.0%\n",
      "Test accuracy: 8.2%\n",
      "Test accuracy: 10.1%\n",
      "Test accuracy: 12.4%\n",
      "Test accuracy: 14.8%\n",
      "Test accuracy: 17.3%\n",
      "Test accuracy: 19.7%\n",
      "Test accuracy: 22.2%\n",
      "Test accuracy: 24.5%\n",
      "Test accuracy: 26.9%\n",
      "Test accuracy: 28.9%\n",
      "Test accuracy: 31.0%\n",
      "Test accuracy: 33.0%\n",
      "Test accuracy: 34.8%\n",
      "Test accuracy: 36.8%\n",
      "Test accuracy: 38.7%\n",
      "Test accuracy: 40.4%\n",
      "Test accuracy: 42.0%\n",
      "Test accuracy: 43.5%\n",
      "Test accuracy: 45.0%\n",
      "Test accuracy: 46.3%\n",
      "Test accuracy: 47.6%\n",
      "Test accuracy: 48.9%\n",
      "Test accuracy: 50.0%\n",
      "Test accuracy: 51.0%\n",
      "Test accuracy: 52.2%\n",
      "Test accuracy: 53.2%\n",
      "Test accuracy: 54.3%\n",
      "Test accuracy: 55.1%\n",
      "Test accuracy: 55.9%\n",
      "Test accuracy: 56.8%\n",
      "Test accuracy: 57.6%\n",
      "Test accuracy: 58.2%\n",
      "Test accuracy: 58.9%\n",
      "Test accuracy: 59.6%\n",
      "Test accuracy: 60.2%\n",
      "Test accuracy: 60.8%\n",
      "Test accuracy: 61.3%\n",
      "Test accuracy: 61.9%\n",
      "Test accuracy: 62.4%\n",
      "Test accuracy: 62.9%\n",
      "Test accuracy: 63.5%\n",
      "Test accuracy: 63.9%\n",
      "Test accuracy: 64.3%\n",
      "Test accuracy: 64.7%\n",
      "Test accuracy: 65.1%\n",
      "Test accuracy: 65.6%\n",
      "Test accuracy: 66.0%\n",
      "Test accuracy: 66.3%\n",
      "Test accuracy: 66.7%\n",
      "Test accuracy: 67.1%\n",
      "Test accuracy: 67.3%\n",
      "Test accuracy: 67.7%\n",
      "Test accuracy: 68.0%\n",
      "Test accuracy: 68.2%\n",
      "Test accuracy: 68.5%\n",
      "Test accuracy: 68.8%\n",
      "Test accuracy: 69.1%\n",
      "Test accuracy: 69.3%\n",
      "Test accuracy: 69.6%\n",
      "Test accuracy: 69.9%\n",
      "Test accuracy: 70.1%\n",
      "Test accuracy: 70.4%\n",
      "Test accuracy: 70.6%\n",
      "Test accuracy: 70.9%\n",
      "Test accuracy: 71.2%\n",
      "Test accuracy: 71.4%\n",
      "Test accuracy: 71.6%\n",
      "Test accuracy: 71.7%\n",
      "Test accuracy: 72.0%\n",
      "Test accuracy: 72.2%\n",
      "Test accuracy: 72.4%\n",
      "Test accuracy: 72.5%\n",
      "Test accuracy: 72.6%\n",
      "Test accuracy: 72.8%\n",
      "Test accuracy: 73.0%\n",
      "Test accuracy: 73.1%\n",
      "Test accuracy: 73.3%\n",
      "Test accuracy: 73.5%\n",
      "Test accuracy: 73.7%\n",
      "Test accuracy: 73.8%\n",
      "Test accuracy: 74.0%\n",
      "Test accuracy: 74.1%\n",
      "Test accuracy: 74.2%\n",
      "Test accuracy: 74.3%\n",
      "Test accuracy: 74.4%\n",
      "Test accuracy: 74.5%\n",
      "Test accuracy: 74.7%\n",
      "Test accuracy: 74.8%\n",
      "Test accuracy: 74.8%\n",
      "Test accuracy: 75.0%\n",
      "Test accuracy: 75.2%\n",
      "Test accuracy: 75.2%\n",
      "Test accuracy: 75.3%\n",
      "Test accuracy: 75.4%\n",
      "Test accuracy: 75.5%\n",
      "Test accuracy: 75.6%\n",
      "Test accuracy: 75.7%\n",
      "Test accuracy: 75.8%\n",
      "Test accuracy: 76.0%\n",
      "Loss at step 100: 1.156897\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 76.5%\n",
      "Test accuracy: 76.0%\n",
      "Test accuracy: 76.1%\n",
      "Test accuracy: 76.3%\n",
      "Test accuracy: 76.4%\n",
      "Test accuracy: 76.5%\n",
      "Test accuracy: 76.5%\n",
      "Test accuracy: 76.6%\n",
      "Test accuracy: 76.7%\n",
      "Test accuracy: 76.9%\n",
      "Test accuracy: 76.9%\n",
      "Test accuracy: 77.0%\n",
      "Test accuracy: 77.2%\n",
      "Test accuracy: 77.3%\n",
      "Test accuracy: 77.3%\n",
      "Test accuracy: 77.4%\n",
      "Test accuracy: 77.5%\n",
      "Test accuracy: 77.7%\n",
      "Test accuracy: 77.7%\n",
      "Test accuracy: 77.8%\n",
      "Test accuracy: 77.9%\n",
      "Test accuracy: 78.0%\n",
      "Test accuracy: 78.1%\n",
      "Test accuracy: 78.2%\n",
      "Test accuracy: 78.3%\n",
      "Test accuracy: 78.3%\n",
      "Test accuracy: 78.4%\n",
      "Test accuracy: 78.5%\n",
      "Test accuracy: 78.6%\n",
      "Test accuracy: 78.7%\n",
      "Test accuracy: 78.8%\n",
      "Test accuracy: 78.8%\n",
      "Test accuracy: 78.9%\n",
      "Test accuracy: 78.9%\n",
      "Test accuracy: 79.0%\n",
      "Test accuracy: 79.1%\n",
      "Test accuracy: 79.1%\n",
      "Test accuracy: 79.2%\n",
      "Test accuracy: 79.3%\n",
      "Test accuracy: 79.4%\n",
      "Test accuracy: 79.4%\n",
      "Test accuracy: 79.4%\n",
      "Test accuracy: 79.5%\n",
      "Test accuracy: 79.5%\n",
      "Test accuracy: 79.6%\n",
      "Test accuracy: 79.7%\n",
      "Test accuracy: 79.7%\n",
      "Test accuracy: 79.7%\n",
      "Test accuracy: 79.8%\n",
      "Test accuracy: 79.9%\n",
      "Test accuracy: 80.0%\n",
      "Test accuracy: 80.0%\n",
      "Test accuracy: 80.1%\n",
      "Test accuracy: 80.2%\n",
      "Test accuracy: 80.2%\n",
      "Test accuracy: 80.2%\n",
      "Test accuracy: 80.3%\n",
      "Test accuracy: 80.3%\n",
      "Test accuracy: 80.4%\n",
      "Test accuracy: 80.5%\n",
      "Test accuracy: 80.5%\n",
      "Test accuracy: 80.6%\n",
      "Test accuracy: 80.6%\n",
      "Test accuracy: 80.6%\n",
      "Test accuracy: 80.6%\n",
      "Test accuracy: 80.7%\n",
      "Test accuracy: 80.7%\n",
      "Test accuracy: 80.8%\n",
      "Test accuracy: 80.9%\n",
      "Test accuracy: 80.9%\n",
      "Test accuracy: 81.0%\n",
      "Test accuracy: 81.0%\n",
      "Test accuracy: 81.1%\n",
      "Test accuracy: 81.1%\n",
      "Test accuracy: 81.2%\n",
      "Test accuracy: 81.2%\n",
      "Test accuracy: 81.2%\n",
      "Test accuracy: 81.2%\n",
      "Test accuracy: 81.3%\n",
      "Test accuracy: 81.4%\n",
      "Test accuracy: 81.4%\n",
      "Test accuracy: 81.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ccfc4cb66d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# and get the loss value and the training predictions returned as numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# will be really slow, so you can decrease num_steps\n",
    "\n",
    "num_steps = 701\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_labels))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1.2 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "As you have experienced, training the model using the full dataset for each update step took a long time... So let's speed things up! We'll now switch to stochastic gradient descent training instead, which is much faster. Now, instead of taking the full training set to compute the error, we only present a small subset of the training data (a.k.a a training batch) during each training step. The weights are then updated according to the error made on this subset of training samples. Although this might seem strange at first sight, if we randomize the batches and use enough iterations, on average the model will have seen all the training examples! Training will go much faster, as you will notice in a moment!  \n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# square images of 28 pixels width and 28 pixels heigt --> 28*28 =784 pixels\n",
    "image_size = 28 \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 12.326206\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 500: 0.836916\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1000: 0.276452\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 1500: 0.642790\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 2000: 0.468029\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 2500: 0.275213\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 3000: 0.076048\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "# SGD needs more iterations than Gradient Descent, however each iteration will go much faster!\n",
    "num_steps = 3001  \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) # accuracy of the used training batch\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That was fast! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "-------\n",
    "\n",
    "Now it's up to you to turn this logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # new hidden layer\n",
    "    hidden_nodes = 1024\n",
    "    hidden_weights = tf.Variable( tf.truncated_normal([image_size * image_size, hidden_nodes]) )\n",
    "    hidden_biases = tf.Variable( tf.zeros([hidden_nodes]))\n",
    "    hidden_layer = tf.nn.relu( tf.matmul( tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable( tf.truncated_normal([hidden_nodes, num_labels])) \n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(hidden_layer, weights) + biases\n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( labels = tf_train_labels, logits = logits) )\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, weights) + biases) \n",
    "\n",
    "    test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-b562655d5bf6>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 312.882629\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 22.7%\n",
      "Minibatch loss at step 500: 0.567973\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 1000: 0.062707\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 1500: 0.137629\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 2000: 0.015088\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 2500: 0.021324\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 3000: 0.004934\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 3500: 0.003097\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 94.4%\n",
      "Minibatch loss at step 4000: 0.001059\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 94.6%\n",
      "Test accuracy: 94.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Regularization\n",
    "\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "              beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-48e980ead88a>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 538.151123\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 18.0%\n",
      "Minibatch loss at step 500: 192.835709\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 1000: 116.794327\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 94.3%\n",
      "Minibatch loss at step 1500: 70.926697\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 95.0%\n",
      "Minibatch loss at step 2000: 43.036919\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 96.1%\n",
      "Minibatch loss at step 2500: 26.132961\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 96.8%\n",
      "Minibatch loss at step 3000: 15.855141\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.0%\n",
      "Test accuracy: 96.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels= tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-b562655d5bf6>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 477.055359\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 500: 1.381874\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 1000: 0.407621\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 1500: 0.705509\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 2000: 0.654068\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 94.0%\n",
      "Minibatch loss at step 2500: 0.456166\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 94.5%\n",
      "Minibatch loss at step 3000: 0.098233\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 94.8%\n",
      "Minibatch loss at step 3500: 0.582637\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 95.2%\n",
      "Minibatch loss at step 4000: 0.291427\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 95.2%\n",
      "Test accuracy: 94.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model!\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size * image_size, num_hidden_nodes1],\n",
    "            stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "        )\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "    logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels= tf_train_labels))\n",
    "  \n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-35d6cc806b04>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.330882\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 500: 0.150851\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 96.4%\n",
      "Minibatch loss at step 1000: 0.025660\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.7%\n",
      "Minibatch loss at step 1500: 0.051042\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 96.8%\n",
      "Minibatch loss at step 2000: 0.019785\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.1%\n",
      "Minibatch loss at step 2500: 0.014415\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.8%\n",
      "Minibatch loss at step 3000: 0.006614\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.3%\n",
      "Minibatch loss at step 3500: 0.004093\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "Minibatch loss at step 4000: 0.032022\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "Minibatch loss at step 4500: 0.001708\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.4%\n",
      "Minibatch loss at step 5000: 0.000276\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "Minibatch loss at step 5500: 0.000194\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "Minibatch loss at step 6000: 0.001033\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 6500: 0.000156\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "Minibatch loss at step 7000: 0.000149\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "Minibatch loss at step 7500: 0.000212\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 8000: 0.000261\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 8500: 0.000256\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 9000: 0.000107\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 9500: 0.000084\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "Minibatch loss at step 10000: 0.000071\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "Minibatch loss at step 10500: 0.000137\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 11000: 0.000316\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "Minibatch loss at step 11500: 0.000292\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-35d6cc806b04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         _, l, predictions = session.run(\n\u001b[0;32m---> 18\u001b[0;31m               [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesboutens/Desktop/virtualEnvPython/pyth2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Convolutional Neural Networks\n",
    "\n",
    "\n",
    "Previously we trained fully connected networks to classify the mnist numbers!\n",
    "However, neural networks really start to shine on image data when we use convolutions. This will be introduced in this part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1 Reshape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll need to reformat our images into a TensorFlow-friendly shape for convolutions!\n",
    "- convolutions need the image data formatted as a cube: width by height by #channels (RGB, SVH,...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (55000, 28, 28, 1), (55000, 10))\n",
      "('Validation set', (5000, 28, 28, 1), (5000, 10))\n",
      "('Test set', (10000, 28, 28, 1), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # we only work with grayscale\n",
    "\n",
    "\n",
    "def reformat(dataset):\n",
    "    \n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = reformat(train_dataset)\n",
    "valid_dataset = reformat(valid_dataset)\n",
    "test_dataset = reformat(test_dataset)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2 First ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.364890\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 50: 2.066435\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 48.1%\n",
      "Minibatch loss at step 100: 0.909678\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 150: 0.556946\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 200: 1.266023\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 250: 0.368713\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 300: 0.525872\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 350: 0.079405\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 400: 0.325029\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 450: 0.190409\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 500: 0.346621\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 550: 0.130683\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 600: 0.196487\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 650: 0.203904\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 700: 0.511764\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 750: 0.714578\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 800: 0.479299\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 850: 0.220738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 900: 0.215043\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 950: 0.441762\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.3%\n",
      "Minibatch loss at step 1000: 0.136661\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.7%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 5\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "        pool1 = tf.nn.max_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "        pool2 = tf.nn.max_pool(bias2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-f10f4db090f5>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.404713\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 9.6%\n",
      "Minibatch loss at step 50: 2.088610\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 26.5%\n",
      "Minibatch loss at step 100: 1.148937\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 150: 0.685320\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 200: 1.181075\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 250: 0.295858\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 300: 0.433954\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 350: 0.048523\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 400: 0.176233\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 450: 0.075218\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 500: 0.350573\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 550: 0.119079\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 94.8%\n",
      "Minibatch loss at step 600: 0.257170\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 650: 0.155178\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 700: 0.128989\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 750: 0.111935\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 95.7%\n",
      "Minibatch loss at step 800: 0.133618\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 95.1%\n",
      "Minibatch loss at step 850: 0.219983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 94.1%\n",
      "Minibatch loss at step 900: 0.112454\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 95.2%\n",
      "Minibatch loss at step 950: 0.463639\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 95.9%\n",
      "Minibatch loss at step 1000: 0.038709\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 95.9%\n",
      "Test accuracy: 95.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
